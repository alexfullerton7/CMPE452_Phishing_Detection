{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Phishing URL Detection\r\n",
    "## Alex Fullerton & John Turnbull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "13458df8-0910-4e9a-a977-6b7025c538f3",
    "_uuid": "a4c6d263-4fa6-47ea-9ab4-658f3aa1f7ae",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 URL  Label\n",
      "0  nobell.it/70ffb52d079109dca5664cce6f317373782/...      1\n",
      "1  www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...      1\n",
      "2  serviciosbys.com/paypal.cgi.bin.get-into.herf....      1\n",
      "3  mail.printakid.com/www.online.americanexpress....      1\n",
      "4  thewhiskeydregs.com/wp-content/themes/widescre...      1\n",
      "\n",
      "Shape: (549346, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# If dataset is too large to run on CPU, try using smaller_phishing_urls.csv\n",
    "txtData = pd.read_csv('./data/phishing_urls.csv')\n",
    "print(txtData.head())\n",
    "print(\"\\nShape:\", txtData.shape)\n",
    "\n",
    "urls = txtData['URL']\n",
    "\n",
    "statuses = txtData['Label']\n",
    "statuses = torch.tensor(statuses)\n",
    "\n",
    "count = 0\n",
    "for url in urls:\n",
    "    if(len(url)>256):\n",
    "        count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7199469915135451\n"
     ]
    }
   ],
   "source": [
    "print((count/549346)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ec9f78e4-d485-4657-b12a-8b90c54b01f9",
    "_uuid": "43ec1d11-b6c6-482b-9492-ed87525386ff",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dictionary = 'abcdefghijklmnopqrstuvwxyz_0123456789-;.!?:/\\\\|#$%^&~’+=<>(),\"’|^'\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "        transforms.ToTensor()  # Convert url images to tensors \n",
    "    ])\n",
    "\n",
    "def one_hot_encode(text, dictionary, target_length):\n",
    "    encoded_text = []\n",
    "    for char in text:\n",
    "        one_hot = [int(char == c) for c in dictionary]\n",
    "        encoded_text.append(one_hot)\n",
    "        \n",
    "    encoded_array = np.array(encoded_text, dtype=np.uint8)\n",
    "    \n",
    "    # Pad or truncate the first dimension to the target length (256)\n",
    "    processed_array = np.pad(encoded_array, ((0, max(0, target_length - encoded_array.shape[0])), (0, 0)), mode='constant')[:, :target_length]\n",
    "    \n",
    "    # Clip the vectors to be at most 256 elements\n",
    "    processed_array = processed_array[:target_length, :]\n",
    "    \n",
    "    # Add channel and batch dimensions\n",
    "    processed_array = processed_array.reshape(1, 1, processed_array.shape[0], processed_array.shape[1])\n",
    "        \n",
    "    # Convert to pixel values (0 and 1) for visualization\n",
    "    pixel_values = (processed_array * 255).astype(np.uint8)\n",
    "    \n",
    "    # Create a PIL Image\n",
    "    encoded_image = Image.fromarray(pixel_values[0, 0].squeeze(), mode='L')\n",
    "    \n",
    "    # Apply image transformation\n",
    "    encoded_image = image_transform(encoded_image)\n",
    "    \n",
    "    return encoded_image\n",
    "\n",
    "\n",
    "one_hot_urls = [one_hot_encode(url, dictionary, 256) for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_set = [(url, status) for url, status in zip(one_hot_urls, statuses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "43d07a6b-2257-4268-a8d9-3453884b5772",
    "_uuid": "f9ea517d-449a-43dd-8d61-254b3815fe02",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-05T01:08:49.820383Z",
     "iopub.status.busy": "2023-12-05T01:08:49.819983Z",
     "iopub.status.idle": "2023-12-05T01:08:49.884918Z",
     "shell.execute_reply": "2023-12-05T01:08:49.883917Z",
     "shell.execute_reply.started": "2023-12-05T01:08:49.820348Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"URL: \", url_set[1][0])\n",
    "print(\"Status: \", url_set[1][1])\n",
    "print(url_set[0][0].shape)\n",
    "      \n",
    "batch_size = 128\n",
    "\n",
    "train_size = int(0.9 * len(url_set))\n",
    "test_size = int(0.05 * len(url_set))\n",
    "valid_size = len(url_set) - train_size - test_size\n",
    "\n",
    "train_set, test_set, valid_set = torch.utils.data.random_split(url_set, [train_size, test_size, valid_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)\n",
    "print(\"# of batches:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "db41cb9a-dfba-48c4-9b62-7fa418b68f68",
    "_uuid": "6da646ff-9d84-4542-b804-ed7904a8f043",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder=None, classifier=None):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 1),\n",
    "            nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 1),\n",
    "            nn.Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2), padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(5, 1),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32*26*2, 2),\n",
    "            nn.Linear(2, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.init_encoder_weights(mean=0.0, std=0.01)\n",
    "\n",
    "        self.init_classifier_weights(mean=0.0, std=0.01)\n",
    "\n",
    "    def init_encoder_weights(self, mean, std):\n",
    "        for param in self.encoder.parameters():\n",
    "            nn.init.normal_(param, mean=mean, std=std)\n",
    "\n",
    "    def init_classifier_weights(self, mean, std):\n",
    "        for param in self.classifier.parameters():\n",
    "            nn.init.normal_(param, mean=mean, std=std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "915424d5-d5d3-483e-b301-9747deac72a8",
    "_uuid": "93ce8664-f32a-4823-873d-a21cf3024e38",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T03:21:54.673950Z",
     "iopub.status.busy": "2023-11-30T03:21:54.673542Z",
     "iopub.status.idle": "2023-11-30T03:21:54.689787Z",
     "shell.execute_reply": "2023-11-30T03:21:54.688496Z",
     "shell.execute_reply.started": "2023-11-30T03:21:54.673918Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, scheduler, loss_fn, train_loader, device,\n",
    "          save_classifier_path, save_encoder_path, save_plot_path):\n",
    "    print(\"training...\")\n",
    "\n",
    "    avg_loss = []\n",
    "    losses_valid = []\n",
    "    epochs = []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print('Epoch', epoch)\n",
    "\n",
    "        # Initialize a new list for this epoch\n",
    "        loss_train = 0.00\n",
    "\n",
    "        data_iter = iter(train_loader)\n",
    "\n",
    "        model.train()  # Keep track of gradient for backtracking\n",
    "\n",
    "        # Iterate through batches\n",
    "        for batch in range(int(len(train_loader))):\n",
    "            urls, labels = next(data_iter)\n",
    "            # Move tensors to the configured device\n",
    "            urls = urls.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass through model\n",
    "            outputs = model(urls)\n",
    "            \n",
    "            loss = loss_fn(outputs.squeeze(1), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        # Calculate the average loss over batches for the entire epoch\n",
    "        avg_loss += [loss_train / len(train_loader)]\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        loss_valid = 0.0\n",
    "        with torch.no_grad():\n",
    "            for urls, labels in valid_loader:\n",
    "                urls = urls.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(urls)\n",
    "                loss = loss_fn(outputs.squeeze(1), labels.float())\n",
    "                loss_valid += loss.item()\n",
    "\n",
    "        # Calculate the average validation loss for the entire epoch\n",
    "        avg_valid_loss = loss_valid / len(valid_loader)\n",
    "        losses_valid.append(avg_valid_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Epoch array for plotting loss\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        print('{} Epoch {}, Training loss {}, Validation loss {}'.format(datetime.datetime.now(), epoch,\n",
    "                                                                         loss_train / len(train_loader),\n",
    "                                                                         loss_valid / len(valid_loader)))\n",
    "\n",
    "    torch.save(model.classifier.state_dict(), save_classifier_path)\n",
    "    torch.save(model.encoder.state_dict(), save_encoder_path)\n",
    "\n",
    "    # Plot training and validation loss over epochs\n",
    "    plt.plot(epochs, avg_loss, label='Training Loss', color='blue')\n",
    "    plt.plot(epochs, losses_valid, label='Validation Loss', color='red')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.savefig(save_plot_path)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "763f544b-d870-4f54-b20e-d938dc1198dd",
    "_uuid": "59c65bc8-99a2-4ac5-9ee2-f0b87219dbf2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "CNN(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=5, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1664, out_features=2, bias=True)\n",
      "    (1): Linear(in_features=2, out_features=1, bias=True)\n",
      "    (2): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 60\n",
    "loss_fn = nn.BCELoss()\n",
    "save_classifier_path = 'CNNclassifier.pth'\n",
    "save_encoder_path = 'CNNencoder.pth'\n",
    "save_plot_path = 'loss.CNN.png'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = CNN()\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=0.0005, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = ExponentialLR(optimizer=optimizer, gamma=0.9)\n",
    "\n",
    "# train(n_epochs, optimizer, model, scheduler, loss_fn, train_loader, device,\n",
    "#           save_classifier_path, save_encoder_path, save_plot_path)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T18:11:55.873818Z",
     "iopub.status.busy": "2023-11-30T18:11:55.873142Z",
     "iopub.status.idle": "2023-11-30T18:11:55.883255Z",
     "shell.execute_reply": "2023-11-30T18:11:55.882354Z",
     "shell.execute_reply.started": "2023-11-30T18:11:55.873770Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def test(model, test_loader, device, loss_fn):\n",
    "    print(\"testing...\")\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    loss_test = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for urls, labels in test_loader:\n",
    "            urls = urls.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(urls)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            loss = loss_fn(outputs, labels.float())\n",
    "            loss_test += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = torch.round(outputs)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            # Collect predictions and targets\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = loss_test / len(test_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    print('{} Test Loss: {}, Accuracy: {:.2%}'.format(datetime.datetime.now(), avg_test_loss, accuracy))\n",
    "    \n",
    "    return all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T18:11:57.567997Z",
     "iopub.status.busy": "2023-11-30T18:11:57.567622Z",
     "iopub.status.idle": "2023-11-30T18:11:58.015806Z",
     "shell.execute_reply": "2023-11-30T18:11:58.014902Z",
     "shell.execute_reply.started": "2023-11-30T18:11:57.567969Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "encoder_path = './CNNencoder.pth'\n",
    "classifier_path = './CNNclassifier.pth'\n",
    "\n",
    "test_model = CNN()\n",
    "\n",
    "encoder_weights = torch.load(encoder_path, map_location=torch.device(device))   \n",
    "classifier_weights = torch.load(classifier_path, map_location=torch.device(device))\n",
    "\n",
    "test_model.to(device)\n",
    "\n",
    "test_model.encoder.load_state_dict(encoder_weights)\n",
    "test_model.classifier.load_state_dict(classifier_weights)\n",
    "\n",
    "preds, labels = test(test_model, test_loader, device, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 791543,
     "sourceId": 1359146,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4083066,
     "sourceId": 7086628,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4087595,
     "sourceId": 7092899,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4087736,
     "sourceId": 7093102,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4110314,
     "sourceId": 7125346,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
